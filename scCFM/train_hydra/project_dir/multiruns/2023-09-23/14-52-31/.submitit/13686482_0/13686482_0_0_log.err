WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1695473589.418696 1816415 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.
  self.seed = seed
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.
  self.dl_pin_memory_gpu_training = (
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/1_OFFICIAL_cfm_eb_latent_geodesic_LEAVEOUT/wandb/run-20230923_145319-b34ktqr2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-butterfly-1
wandb: ⭐️ View project at https://wandb.ai/allepalma/1_OFFICIAL_cfm_eb_latent_geodesic_leaveout
wandb: 🚀 View run at https://wandb.ai/allepalma/1_OFFICIAL_cfm_eb_latent_geodesic_leaveout/runs/b34ktqr2
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type      | Params
----------------------------------------
0 | net       | MLP       | 9.9 K 
1 | node      | NeuralODE | 9.9 K 
2 | criterion | MSELoss   | 0     
----------------------------------------
9.9 K     Trainable params
0         Non-trainable params
9.9 K     Total params
0.039     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
slurmstepd: error: *** JOB 13686485 ON gpusrv55 CANCELLED AT 2023-09-23T14:55:52 ***
slurmstepd: error: *** STEP 13686485.0 ON gpusrv55 CANCELLED AT 2023-09-23T14:55:52 ***
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
submitit WARNING (2023-09-23 14:55:52,083) - Bypassing signal SIGTERM
submitit WARNING (2023-09-23 14:55:52,084) - Bypassing signal SIGCONT
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.014 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: | 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:           train/loss ▅█▆▄▅▃▄▂▂▁▄
wandb:  trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             val/loss █▇▅▄▆▃▅▄▄▄▄▄▄▃▃▄▃▃▂▂▂▃▃▂▃▂▂▁▃▂▂▂▂▂▂▁▁▁▂▁
wandb: val/t1/1-Wasserstein █▃▃▃▄▆▂▃▃▃▃▅▂▃▃▃▃▃▄▄▃▅▃▃▃▃▄▄▁▄▄▅▂▃▃▂▃▄▃▄
wandb: val/t1/2-Wasserstein █▃▃▃▃▅▂▃▃▃▃▅▂▃▃▃▃▂▄▃▃▄▃▃▃▃▄▃▁▄▄▄▂▃▃▂▂▃▃▃
wandb:    val/t1/Linear_MMD █▂▂▃▃▆▁▃▃▂▂▅▃▂▄▃▃▂▄▄▃▅▄▄▃▃▄▄▁▄▅▆▄▅▄▂▃▄▄▄
wandb:       val/t1/Mean_L1 █▂▁▃▇▄▃▄▃▂▃▆▃▃▄▄▁▃▄▆▂▄▃▅▃▃▃▆▁▄▆▅▃▃▂▂▄▄▃▃
wandb:       val/t1/Mean_L2 █▁▁▃▅▅▂▄▃▃▃▆▃▃▄▃▃▂▅▅▂▄▄▅▃▃▃▅▁▄▆▄▃▅▂▁▃▄▃▃
wandb:      val/t1/Mean_MSE █▁▁▃▅▅▂▄▃▂▂▆▃▃▄▃▂▂▄▅▂▄▃▅▃▂▃▅▁▃▅▄▃▄▂▁▃▄▃▃
wandb:     val/t1/Median_L1 ▁▇▅▆▅▅▅▆▅▅▅▃▅▄▅▄▆▅▅▄▅▄▅█▄█▇▇▅▂▄▄▃▆▂▅▄▂▃▅
wandb:     val/t1/Median_L2 ▂▆▆▆▆▄▃▆▄▅▄▃▄▅▄▄▆▆▅▂▅▃▅█▄█▅▆▅▂▄▃▃▆▁▄▄▂▄▄
wandb:    val/t1/Median_MSE ▂▆▆▆▆▄▃▅▅▄▅▂▄▅▅▄▆▆▅▁▅▄▅█▅█▆▇▅▃▄▃▃▆▁▄▄▂▅▅
wandb:      val/t1/Poly_MMD █▂▃▃▃▆▁▃▃▃▃▆▃▂▄▃▃▂▅▄▃▅▄▄▃▃▄▅▁▄▅▆▄▅▄▂▃▄▄▄
wandb:       val/t1/RBF_MMD █▁▁▃▄▇▁▂▄▂▃▆▂▁▅▃▃▃▆▅▃▆▄▅▄▄▆▆▁▃▆▇▆▅▄▄▅▆▆▅
wandb: val/t2/1-Wasserstein ▁▇▅▆▅▅▅▆▅▅▅▃▅▄▅▄▆▆▅▄▅▄▅█▅█▇█▆▂▅▄▃▆▂▅▄▂▃▅
wandb: val/t2/2-Wasserstein ▁▆▅▆▅▆▆▆▆▆▅▅▅▅▆▅▇▆▆▆▆▅▆█▄█▆▆▇▂▅▅▃▆▃▅▄▃▅▅
wandb:    val/t2/Linear_MMD ▁▅▅▆▇▅▄▆▆▆▄▃▅▅▆▄▅▄▆▄▅▄▅█▅█▆▆▆▂▅▄▃▆▂▃▅▃▃▄
wandb:       val/t2/Mean_L1 ▆█▄▇▄▄▅▅▅▃▄▃▃▂▃▁▄▃▂▂▆▃▁▃▃▅▂▂▄▄▃▂▂▂▃▁▁▂▂▁
wandb:       val/t2/Mean_L2 ▆█▄█▃▆▆▅▅▂▅▅▃▄▄▅▅▂▃▅▄▄▃▄▃▅▂▃▆▄▃▂▂▂▄▁▂▃▂▂
wandb:      val/t2/Mean_MSE ▆▇▄█▃▆▆▅▆▂▅▅▃▄▄▅▅▂▃▅▄▅▃▄▃▅▂▄▅▄▃▂▂▂▃▁▃▃▂▂
wandb:     val/t2/Median_L1 █▇▄▇▅▅▆▅▆▃▄▆▄▃▅▁▆▃▃▄▇▄▂▅▆▆▂▂▅▅▄▁▂▂▅▁▂▂▂▁
wandb:     val/t2/Median_L2 ██▅▇▄▅▆▅▆▄▄▅▄▂▄▁▅▃▂▃▆▄▁▄▄▆▃▂▅▅▄▂▂▂▅▁▁▃▂▁
wandb:    val/t2/Median_MSE ▆█▅▇▅▅▆▆▆▄▄▄▄▃▄▁▅▃▃▃▆▄▂▄▄▆▂▂▄▅▄▂▂▂▄▁▁▃▃▂
wandb:      val/t2/Poly_MMD ▁▅▅▆▇▅▅▆▆▆▅▄▅▅▆▄▅▅▆▄▅▅▅█▅█▆▆▆▂▅▅▃▆▂▄▆▄▃▄
wandb:       val/t2/RBF_MMD ▁▅▅▆▆▆▅▇▆▇▅▅▅▅▇▄▅▅▆▅▆▅▅█▄▇▆▅▆▂▅▆▂▄▂▃▅▃▃▅
wandb: val/t3/1-Wasserstein █▇▅▇▆▆▇▆▇▄▅▆▅▄▅▁▆▃▄▄█▅▂▅▆▆▃▂▆▆▅▂▂▃▅▁▃▃▃▁
wandb: val/t3/2-Wasserstein █▆▄▆▄▅▅▃▆▃▄▆▄▃▄▁▅▃▃▅▆▄▂▄▅▅▃▂▅▄▅▂▂▃▄▂▂▃▂▂
wandb:    val/t3/Linear_MMD █▆▆▆▆▅▄▆▅▆▅▅▄▄▃▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂
wandb:       val/t3/Mean_L1 █▃▄▅▅▃▂▃▄▃▅▄▂▄▂▄▃▃▃▃▂▂▄▂▃▂▃▁▂▃▂▃▂▄▂▂▂▁▃▃
wandb:       val/t3/Mean_L2 █▅▅▅▆▃▃▄▅▃▅▅▁▄▂▄▂▃▃▄▁▂▄▃▃▁▃▁▁▃▃▃▂▃▁▂▃▁▃▃
wandb:      val/t3/Mean_MSE █▄▅▅▆▃▂▄▄▃▄▅▁▄▁▄▂▃▂▃▁▂▄▃▃▁▂▁▁▂▂▃▁▃▂▂▃▁▃▃
wandb:     val/t3/Median_L1 █▆▅▇▅▆▃▅▅▄▅▅▃▄▄▄▄▃▃▃▃▄▃▄▃▄▃▄▂▃▃▃▁▃▁▁▂▂▂▂
wandb:     val/t3/Median_L2 █▄▄▅▆▄▂▅▅▄▆▅▂▄▂▃▃▃▃▄▂▃▄▃▄▂▃▁▃▄▃▃▄▅▂▃▄▂▃▃
wandb:    val/t3/Median_MSE █▄▅▅▆▄▃▄▅▃▆▅▃▅▂▄▄▃▃▄▂▃▅▃▄▂▄▁▃▄▂▃▃▅▃▃▃▂▄▄
wandb:      val/t3/Poly_MMD █▆▆▆▆▅▄▅▅▅▅▅▄▄▃▄▃▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂
wandb:       val/t3/RBF_MMD █▃▄▄▅▃▂▃▄▂▃▄▁▃▁▃▂▂▂▃▁▂▃▂▂▁▂▁▁▂▂▂▁▂▁▁▂▁▂▂
wandb: val/t4/1-Wasserstein █▆▅▇▅▆▄▅▅▄▄▅▃▄▃▄▄▃▃▃▃▃▃▃▃▄▃▃▃▂▂▂▁▃▁▁▂▁▂▂
wandb: val/t4/2-Wasserstein █▅▃▆▄▆▂▄▄▂▃▅▂▁▃▂▄▃▃▃▃▄▃▆▃▅▅▅▁▂▄▄▁▅▁▁▂▁▂▃
wandb:    val/t4/Linear_MMD █▆▅▇▆▆▃▅▅▃▄▅▂▃▃▂▄▃▃▃▄▄▃▅▃▄▄▃▂▃▃▃▁▄▂▁▂▂▃▃
wandb:       val/t4/Mean_L2 █▄▄▆▇▅▄▅▆▄▅▆▃▃▄▂▃▃▄▅▄▄▃▆▄▅▃▃▄▃▅▄▂▃▁▁▄▂▂▂
wandb:      val/t4/Mean_MSE █▅▄▆▇▅▄▆▆▄▅▆▃▄▄▃▄▃▄▅▄▄▄▆▅▅▄▃▃▃▅▃▂▅▂▁▃▂▃▂
wandb:      val/t4/Poly_MMD █▆▄▇▆▆▄▅▆▃▄▆▂▂▄▂▄▃▃▄▃▄▃▅▃▅▄▃▃▁▄▄▁▃▂▁▂▂▃▂
wandb:       val/t4/RBF_MMD █▄▃▆█▅▃▆▆▄▄▆▃▄▅▃▄▃▅▅▄▄▄▇▅▆▅▅▃▃▆▄▂▆▁▁▄▃▃▃
wandb: 
wandb: Run summary:
wandb:                epoch 45
wandb:           train/loss 0.07765
wandb:  trainer/global_step 597
wandb:             val/loss 0.07492
wandb: val/t1/1-Wasserstein 1.26007
wandb: val/t1/2-Wasserstein 1.3022
wandb:    val/t1/Linear_MMD 0.04666
wandb:       val/t1/Mean_L1 0.20992
wandb:       val/t1/Mean_L2 0.24806
wandb:      val/t1/Mean_MSE 0.06153
wandb:     val/t1/Median_L1 0.03873
wandb:     val/t1/Median_L2 1.26331
wandb:    val/t1/Median_MSE 1.21897
wandb:      val/t1/Poly_MMD 0.21602
wandb:       val/t1/RBF_MMD 0.18696
wandb: val/t2/1-Wasserstein 0.19679
wandb: val/t2/2-Wasserstein 0.16477
wandb:    val/t2/Linear_MMD 0.04652
wandb:       val/t2/Mean_L1 0.00185
wandb:       val/t2/Mean_L2 0.89341
wandb:      val/t2/Mean_MSE 0.8574
wandb:     val/t2/Median_L1 0.00216
wandb:     val/t2/Median_L2 0.03386
wandb:    val/t2/Median_MSE 0.043
wandb:      val/t2/Poly_MMD 0.21568
wandb:       val/t2/RBF_MMD 0.18994
wandb: val/t3/1-Wasserstein 0.04648
wandb: val/t3/2-Wasserstein 0.0371
wandb:    val/t3/Linear_MMD 0.89348
wandb:       val/t3/Mean_L1 0.00537
wandb:       val/t3/Mean_L2 0.0384
wandb:      val/t3/Mean_MSE 0.05036
wandb:     val/t3/Median_L1 1.05748
wandb:     val/t3/Median_L2 0.05485
wandb:    val/t3/Median_MSE 0.07325
wandb:      val/t3/Poly_MMD 0.91688
wandb:       val/t3/RBF_MMD 0.00254
wandb: val/t4/1-Wasserstein 1.09395
wandb: val/t4/2-Wasserstein 0.02244
wandb:    val/t4/Linear_MMD 0.12654
wandb:       val/t4/Mean_L2 0.12295
wandb:      val/t4/Mean_MSE 0.14587
wandb:      val/t4/Poly_MMD 0.106
wandb:       val/t4/RBF_MMD 0.02889
wandb: 
wandb: 🚀 View run lemon-butterfly-1 at: https://wandb.ai/allepalma/1_OFFICIAL_cfm_eb_latent_geodesic_leaveout/runs/b34ktqr2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/1_OFFICIAL_cfm_eb_latent_geodesic_LEAVEOUT/wandb/run-20230923_145319-b34ktqr2/logs
