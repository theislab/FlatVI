{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9a81f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.\n",
      "  self.seed = seed\n",
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.\n",
      "  self.dl_pin_memory_gpu_training = (\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import pytorch_lightning as pl\n",
    "import seml\n",
    "import torch\n",
    "from sacred import SETTINGS, Experiment\n",
    "from functools import partial\n",
    "\n",
    "sys.path.insert(0,\"../\")\n",
    "from paths import EXPERIMENT_FOLDER\n",
    "\n",
    "from scCFM.datamodules.time_sc_datamodule import TrajectoryDataModule\n",
    "from scCFM.models.cfm.cfm_module import CFMLitModule\n",
    "from scCFM.models.cfm.components.mlp import MLP\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037b8b4",
   "metadata": {},
   "source": [
    "## Import configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d124bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"training\": {\n",
    "        \"task_name\": \"1_OFFICIAL_cfm_eb_latent_vae\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"datamodule\": {\n",
    "        \"path\": \"/nfs/homedirs/pala/scCFM/project_dir/data/eb/flat/eb_lib.h5ad\",\n",
    "        \"x_layer\": \"X_latents\",\n",
    "        \"time_key\": \"experimental_time\",\n",
    "        \"use_pca\": False,\n",
    "        \"n_dimensions\": None,\n",
    "        \"train_val_test_split\": [0.90, 0.1],\n",
    "        \"num_workers\": 2,\n",
    "        \"batch_size\": 256,\n",
    "        \"model_library_size\": True\n",
    "    },\n",
    "    \"net\": {\n",
    "        \"w\": 64,\n",
    "        \"time_varying\": True\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ot_sampler\": \"exact\",\n",
    "        \"sigma\": 0.1,\n",
    "        \"use_real_time\": False,\n",
    "        \"lr\": 0.001,\n",
    "        \"antithetic_time_sampling\": False, \n",
    "        \"leaveout_timepoint\": 1,\n",
    "    },\n",
    "    \"model_checkpoint\": {\n",
    "        \"filename\": \"epoch_{epoch:04d}\",\n",
    "        \"monitor\": \"train/loss\",\n",
    "        \"mode\": \"min\",\n",
    "        \"save_last\": True,\n",
    "        \"auto_insert_metric_name\": False\n",
    "    },\n",
    "    \"early_stopping\": {\n",
    "        \"perform_early_stopping\": False,\n",
    "        \"monitor\": \"train/loss\",\n",
    "        \"patience\": 200,\n",
    "        \"mode\": \"min\",\n",
    "        \"min_delta\": 0.0,\n",
    "        \"verbose\": False,\n",
    "        \"strict\": True,\n",
    "        \"check_finite\": True,\n",
    "        \"stopping_threshold\": None,\n",
    "        \"divergence_threshold\": None,\n",
    "        \"check_on_train_epoch_end\": None\n",
    "    },\n",
    "    \"logger\": {\n",
    "        \"offline\": True,\n",
    "        \"id\": None,\n",
    "        \"project\": \"1_OFFICIAL_cfm_eb_latent_vae\",\n",
    "        \"log_model\": False,\n",
    "        \"prefix\": \"\",\n",
    "        \"group\": \"\",\n",
    "        \"tags\": [],\n",
    "        \"job_type\": \"\"\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": None,\n",
    "        \"max_steps\": 20000,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"log_every_n_steps\": 50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8ce8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_wrapper(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.model(torch.cat([x, t.repeat(x.shape[0])[:, None]], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f12d4",
   "metadata": {},
   "source": [
    "## Initialize and train/load autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5662d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function \n",
    "class Solver:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def init_general(self, \n",
    "                     task_name,\n",
    "                     seed):\n",
    "        \n",
    "        self.task_name = task_name \n",
    "        \n",
    "        # Fix seed for reproducibility\n",
    "        torch.manual_seed(seed)      \n",
    "        if seed: \n",
    "            pl.seed_everything(seed, workers=True)\n",
    "            \n",
    "        # Initialize folder \n",
    "        self.current_experiment_dir = EXPERIMENT_FOLDER / self.task_name\n",
    "        self.current_experiment_dir.mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    def init_datamodule(self, \n",
    "                        path,\n",
    "                        x_layer, \n",
    "                        time_key,\n",
    "                        use_pca,\n",
    "                        n_dimensions, \n",
    "                        train_val_test_split,\n",
    "                        batch_size,\n",
    "                        num_workers, \n",
    "                        model_library_size):\n",
    "        \n",
    "        # Initialize datamodule\n",
    "        self.datamodule = TrajectoryDataModule(path=path,\n",
    "                                               x_layer=x_layer,\n",
    "                                               time_key=time_key,\n",
    "                                               use_pca=use_pca,\n",
    "                                               n_dimensions=n_dimensions,\n",
    "                                               train_val_test_split=train_val_test_split,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=num_workers, \n",
    "                                               model_library_size=model_library_size)\n",
    "         \n",
    "    def init_net(self, \n",
    "                 w,\n",
    "                 time_varying):\n",
    "        \n",
    "        # Neural network \n",
    "        net_hparams = {\"dim\": self.datamodule.dim,\n",
    "                        \"w\": w,\n",
    "                        \"time_varying\": time_varying}\n",
    "        \n",
    "        self.net = MLP(**net_hparams) \n",
    "\n",
    "    def init_model(self,\n",
    "                   ot_sampler,\n",
    "                   sigma,\n",
    "                   lr,\n",
    "                   use_real_time, \n",
    "                   antithetic_time_sampling, \n",
    "                   leaveout_timepoint):\n",
    "        \n",
    "        # Initialize the model \n",
    "        self.model = CFMLitModule(\n",
    "                            net=self.net,\n",
    "                            datamodule=self.datamodule,\n",
    "                            ot_sampler=ot_sampler, \n",
    "                            sigma=sigma, \n",
    "                            lr=lr, \n",
    "                            use_real_time=use_real_time,\n",
    "                            antithetic_time_sampling=antithetic_time_sampling,\n",
    "                            leaveout_timepoint=leaveout_timepoint) \n",
    "        \n",
    "    def init_checkpoint_callback(self, \n",
    "                                 filename, \n",
    "                                 monitor,\n",
    "                                 mode,\n",
    "                                 save_last,\n",
    "                                 auto_insert_metric_name):\n",
    "        \n",
    "        # Initialize callbacks \n",
    "        self.model_ckpt_callbacks = ModelCheckpoint(dirpath=self.current_experiment_dir / \"checkpoints\", \n",
    "                                                    filename=filename,\n",
    "                                                    monitor=monitor,\n",
    "                                                    mode=mode,\n",
    "                                                    save_last=save_last,\n",
    "                                                    auto_insert_metric_name=auto_insert_metric_name)\n",
    "    \n",
    "    def init_early_stopping_callback(self, \n",
    "                                     perform_early_stopping,\n",
    "                                     monitor, \n",
    "                                     patience, \n",
    "                                     mode,\n",
    "                                     min_delta,\n",
    "                                     verbose,\n",
    "                                     strict, \n",
    "                                     check_finite,\n",
    "                                     stopping_threshold,\n",
    "                                     divergence_threshold,\n",
    "                                     check_on_train_epoch_end):\n",
    "        \n",
    "        # Initialize callbacks \n",
    "        if perform_early_stopping:\n",
    "            self.early_stopping_callbacks = EarlyStopping(monitor=monitor,\n",
    "                                                        patience=patience, \n",
    "                                                        mode=mode,\n",
    "                                                        min_delta=min_delta,\n",
    "                                                        verbose=verbose,\n",
    "                                                        strict=strict,\n",
    "                                                        check_finite=check_finite,\n",
    "                                                        stopping_threshold=stopping_threshold,\n",
    "                                                        divergence_threshold=divergence_threshold,\n",
    "                                                        check_on_train_epoch_end=check_on_train_epoch_end\n",
    "                                                        )\n",
    "        else:\n",
    "            self.early_stopping_callbacks = None\n",
    "        \n",
    "    def init_logger(self, \n",
    "                    offline, \n",
    "                    id, \n",
    "                    project, \n",
    "                    log_model, \n",
    "                    prefix, \n",
    "                    group, \n",
    "                    tags, \n",
    "                    job_type):\n",
    "        \n",
    "        # Initialize logger \n",
    "        self.logger = WandbLogger(save_dir=self.current_experiment_dir, \n",
    "                                  offline=offline,\n",
    "                                  id=id, \n",
    "                                  project=project,\n",
    "                                  log_model=log_model, \n",
    "                                  prefix=prefix,\n",
    "                                  group=group,\n",
    "                                  tags=tags,\n",
    "                                  job_type=job_type) \n",
    "        \n",
    "    def init_trainer(self, \n",
    "                     max_epochs,\n",
    "                     max_steps,\n",
    "                     accelerator,\n",
    "                     devices, \n",
    "                     log_every_n_steps):    \n",
    "        # Initialize the lightning trainer \n",
    "        self.trainer = Trainer(default_root_dir=self.current_experiment_dir,\n",
    "                                  max_epochs=max_epochs,\n",
    "                                  max_steps=max_steps,\n",
    "                                  accelerator=accelerator,\n",
    "                                  devices=devices,\n",
    "                                  log_every_n_steps=log_every_n_steps)\n",
    "                \n",
    "    def train(self):\n",
    "        # Fit the model \n",
    "        self.trainer.fit(model=self.model, \n",
    "                          train_dataloaders=self.datamodule.train_dataloader(),\n",
    "                          val_dataloaders=self.datamodule.val_dataloader())\n",
    "        \n",
    "        train_metrics = self.trainer.callback_metrics\n",
    "        return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6622af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ed17e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n",
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "solver.init_general(**config[\"training\"])\n",
    "solver.init_datamodule(**config[\"datamodule\"])\n",
    "solver.init_net(**config[\"net\"])\n",
    "solver.init_model(**config[\"model\"])\n",
    "solver.init_trainer(**config[\"trainer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf423ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python ...\n",
      "  rank_zero_warn(\n",
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /nfs/homedirs/pala/scCFM/project_dir/experiments/1_OFFICIAL_cfm_eb_latent_vae/lightning_logs/version_8502872/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | net       | MLP       | 9.9 K \n",
      "1 | node      | NeuralODE | 9.9 K \n",
      "2 | criterion | MSELoss   | 0     \n",
      "----------------------------------------\n",
      "9.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.9 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06848901b2a494cbd26b6d19fc20438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n",
      "0 2\n",
      "2 3\n",
      "3 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e61f1a7e9841ada7041f35fe1cc819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "2 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-ssd/pala/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train/loss': tensor(0.4473, device='cuda:0'),\n",
       " 'val/loss': tensor(0.4791, device='cuda:0'),\n",
       " 'val/t1/1-Wasserstein': tensor(2.6931, device='cuda:0'),\n",
       " 'val/t1/2-Wasserstein': tensor(2.7870, device='cuda:0'),\n",
       " 'val/t1/Linear_MMD': tensor(0.0945, device='cuda:0'),\n",
       " 'val/t1/Poly_MMD': tensor(0.3074, device='cuda:0'),\n",
       " 'val/t1/RBF_MMD': tensor(0.2460, device='cuda:0'),\n",
       " 'val/t1/Mean_MSE': tensor(0.1355, device='cuda:0'),\n",
       " 'val/t1/Mean_L2': tensor(0.3682, device='cuda:0'),\n",
       " 'val/t1/Mean_L1': tensor(0.2791, device='cuda:0'),\n",
       " 'val/t1/Median_MSE': tensor(2.9048, device='cuda:0'),\n",
       " 'val/t1/Median_L2': tensor(3.0316, device='cuda:0'),\n",
       " 'val/t1/Median_L1': tensor(0.1730, device='cuda:0'),\n",
       " 'val/t2/1-Wasserstein': tensor(0.4159, device='cuda:0'),\n",
       " 'val/t2/2-Wasserstein': tensor(0.3283, device='cuda:0'),\n",
       " 'val/t2/Linear_MMD': tensor(0.2271, device='cuda:0'),\n",
       " 'val/t2/Poly_MMD': tensor(0.4766, device='cuda:0'),\n",
       " 'val/t2/RBF_MMD': tensor(0.4108, device='cuda:0'),\n",
       " 'val/t2/Mean_MSE': tensor(2.5310, device='cuda:0'),\n",
       " 'val/t2/Mean_L2': tensor(2.7010, device='cuda:0'),\n",
       " 'val/t2/Mean_L1': tensor(0.0918, device='cuda:0'),\n",
       " 'val/t2/Median_MSE': tensor(0.3030, device='cuda:0'),\n",
       " 'val/t2/Median_L2': tensor(0.2340, device='cuda:0'),\n",
       " 'val/t2/Median_L1': tensor(0.0698, device='cuda:0'),\n",
       " 'val/t3/1-Wasserstein': tensor(0.2643, device='cuda:0'),\n",
       " 'val/t3/2-Wasserstein': tensor(0.2136, device='cuda:0'),\n",
       " 'val/t3/Linear_MMD': tensor(2.5504, device='cuda:0'),\n",
       " 'val/t3/Poly_MMD': tensor(2.6922, device='cuda:0'),\n",
       " 'val/t3/RBF_MMD': tensor(0.0604, device='cuda:0'),\n",
       " 'val/t3/Mean_MSE': tensor(0.2458, device='cuda:0'),\n",
       " 'val/t3/Mean_L2': tensor(0.1697, device='cuda:0'),\n",
       " 'val/t3/Mean_L1': tensor(0.0704, device='cuda:0'),\n",
       " 'val/t3/Median_MSE': tensor(0.2653, device='cuda:0'),\n",
       " 'val/t3/Median_L2': tensor(0.1885, device='cuda:0'),\n",
       " 'val/t3/Median_L1': tensor(2.6698, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/1-Wasserstein': tensor(2.8030, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/2-Wasserstein': tensor(0.1049, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/Linear_MMD': tensor(0.3180, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/Poly_MMD': tensor(0.2445, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/RBF_MMD': tensor(0.1257, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/Mean_MSE': tensor(0.3436, device='cuda:0', dtype=torch.float64),\n",
       " 'val/t4/Mean_L2': tensor(0.2730, device='cuda:0', dtype=torch.float64)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb45aad-bd70-43fd-a119-e2154cfc4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52b680-248b-4d20-85c5-8ddea3ddd448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a6396-8a74-4cff-9f72-2b7d2a63394d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5240001f602d094a653e72667f1f126473feea48647f48d8a8ce2a6fb0c39cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
