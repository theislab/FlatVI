WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1711225964.022115 2790682 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.
  self.seed = seed
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.
  self.dl_pin_memory_gpu_training = (
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /ictstr01/home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/eb_geodesic/wandb/run-20240323_213250-rfd2soe8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-forest-1
wandb: ⭐️ View project at https://wandb.ai/allepalma/eb_geodesic
wandb: 🚀 View run at https://wandb.ai/allepalma/eb_geodesic/runs/rfd2soe8
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../scCFM/train_hydra/train_cfm.py checkpoint=eb_geod ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python ../scCFM/train_hydra/train_cfm.py checkpoint=eb_geod ...
  rank_zero_warn(
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type      | Params
----------------------------------------
0 | net       | MLP       | 9.9 K 
1 | node      | NeuralODE | 9.9 K 
2 | criterion | MSELoss   | 0     
----------------------------------------
9.9 K     Trainable params
0         Non-trainable params
9.9 K     Total params
0.039     Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('leaveout_timepoint', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:   leaveout_timepoint ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train/loss █▆▄▅▄▄▄▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▂▂▂▁▂▁▁▁▂▁▂▂▁
wandb:  trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:             val/loss █▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁
wandb: val/t1/1-Wasserstein █▇▆▅▄▄▄▄▃▃▃▃▅▂▂▂▄▂▃▂▂▁▁▃▁▂▂▂▁▂▁▂▂▂▂▁▁▂▂▁
wandb: val/t1/2-Wasserstein █▇▅▅▄▄▄▄▃▃▃▃▅▂▂▂▄▂▃▂▂▁▁▃▂▂▂▂▁▂▁▂▂▂▁▁▁▂▁▁
wandb:    val/t1/Linear_MMD ▃▃▂▁▂▃▂▃▂▃▂▂█▁▂▂▅▂▃▂▂▂▁▅▂▂▂▂▁▄▂▃▃▃▁▁▁▃▂▃
wandb:       val/t1/Mean_L1 ▆▄▄▃▄▄▃▄▃▄▃▃█▂▃▃▆▃▄▃▃▃▁▆▂▂▄▄▂▅▃▄▄▂▁▂▃▅▃▃
wandb:       val/t1/Mean_L2 ▆▄▄▃▄▄▃▄▃▄▂▃█▂▃▃▆▃▄▃▂▃▁▇▂▃▄▄▂▆▃▄▄▃▁▂▃▆▃▄
wandb:      val/t1/Mean_MSE ▅▃▃▂▃▃▂▃▂▃▂▂█▂▂▂▅▂▃▂▂▂▁▆▂▂▃▃▂▅▂▃▃▂▁▁▂▅▂▃
wandb:     val/t1/Median_L1 █▇▃▂▁▁▁▂▂▂▁▁▁▂▃▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▂▃▄▁▁▁▂▃▁▂
wandb:     val/t1/Median_L2 ██▆▅▄▃▄▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▁▂▂▁▂▂▂▂▁▂
wandb:    val/t1/Median_MSE ██▆▅▄▃▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▁▂▂▂▂▁▂
wandb:      val/t1/Poly_MMD ▄▄▂▂▃▃▃▄▃▄▂▃█▂▂▃▆▃▄▂▃▂▁▆▂▃▃▃▂▅▃▄▄▄▂▂▂▄▃▄
wandb:       val/t1/RBF_MMD ▄▄▃▂▃▃▂▄▃▃▂▃█▂▂▃▆▂▄▂▃▂▁▆▂▂▃▃▁▄▃▃▄▃▁▁▂▄▂▃
wandb: val/t2/1-Wasserstein █▇▃▂▂▂▂▃▂▂▁▁▁▂▃▃▂▂▃▂▃▃▂▂▂▃▁▁▂▃▂▄▅▁▂▂▃▄▁▃
wandb: val/t2/2-Wasserstein █▇▄▂▂▂▂▃▂▂▁▁▁▂▄▃▂▂▂▂▃▃▂▂▂▃▁▁▃▃▂▄▄▁▂▂▃▃▁▂
wandb:    val/t2/Linear_MMD █▆▄▃▂▂▃▂▂▃▂▂▁▂▂▂▂▃▂▂▃▂▂▁▂▃▂▂▂▂▂▄▄▁▂▂▂▃▁▂
wandb:       val/t2/Mean_L1 █▄▂▁▃▂▂▃▃▁▁▂▂▂▂▂▂▁▂▁▂▁▂▂▁▂▁▁▂▂▁▁▄▂▂▁▁▂▁▂
wandb:       val/t2/Mean_L2 █▄▄▄▄▃▃▃▄▃▂▃▂▂▂▃▂▂▂▂▃▂▂▂▂▂▁▂▂▂▁▁▄▂▂▂▂▂▁▂
wandb:      val/t2/Mean_MSE █▄▄▄▄▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▃▂▂▂▂▂▁▂▂▂▁▂▄▂▂▁▂▂▁▃
wandb:     val/t2/Median_L1 █▅▂▁▃▂▃▃▃▁▁▃▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▂▂▁▂▄▁▂▂▁▂▂▂
wandb:     val/t2/Median_L2 █▅▂▂▅▃▃▄▄▁▂▃▃▃▃▃▂▂▃▁▃▂▂▃▂▃▂▂▃▃▂▂▅▂▃▂▂▃▂▂
wandb:    val/t2/Median_MSE █▅▂▂▅▃▃▄▄▂▂▃▃▃▃▃▃▂▃▁▃▂▂▃▂▃▁▂▃▃▂▂▅▂▃▂▂▃▂▂
wandb:      val/t2/Poly_MMD █▇▅▄▃▃▄▃▃▃▃▃▁▃▃▃▂▄▃▃▄▃▂▂▃▄▂▃▃▂▂▅▅▁▃▂▃▄▁▂
wandb:       val/t2/RBF_MMD █▇▅▃▂▃▄▃▃▃▃▃▂▂▃▄▂▂▃▃▃▂▃▂▃▄▂▃▃▂▂▄▅▁▃▂▃▃▁▂
wandb: val/t3/1-Wasserstein █▆▃▁▄▃▄▄▄▁▂▄▃▃▃▃▃▁▃▁▃▂▂▃▂▂▁▂▂▃▂▂▅▂▃▂▁▂▂▃
wandb: val/t3/2-Wasserstein █▅▃▁▄▃▄▃▄▁▁▄▃▂▃▃▃▁▂▁▃▂▁▃▂▂▁▂▂▃▁▂▅▂▃▂▁▂▂▂
wandb:    val/t3/Linear_MMD █▅▄▄▄▃▃▄▂▃▂▃▂▃▃▂▂▂▂▁▂▃▂▂▂▃▂▂▁▂▃▁▂▂▂▁▃▃▁▂
wandb:       val/t3/Mean_L1 ▃▂█▅▄▅▃▅▂▄▂▅▁▂▃▃▂▂▁▁▃▆▂▄▄▄▂▃▁▂▃▂▄▂▃▁▄▆▁▃
wandb:       val/t3/Mean_L2 ▄▂▇▅▅▅▃█▂▃▃▅▃▅▄▄▁▃▁▁▆▇▄▃▅▅▄▅▂▂▆▃▅▃▅▁██▃▃
wandb:      val/t3/Mean_MSE ▄▂▇▅▆▅▃▇▂▃▂▅▃▄▅▃▂▃▂▁▅▇▃▃▅▅▄▅▃▂▆▃▅▄▅▁▇█▃▃
wandb:     val/t3/Median_L1 █▆▅▄▄▃▃▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▁▂▂▂▁▂▂▁▁
wandb:     val/t3/Median_L2 ▅▂█▅▅▆▄▇▂▅▄▆▂▄▃▅▂▁▂▁▄█▂▅▆▅▂▄▂▃▄▃▆▃▄▁▆█▁▃
wandb:    val/t3/Median_MSE ▄▂█▆▅▆▄▆▃▅▃▆▂▃▄▄▃▂▂▁▄▆▂▅▅▅▃▄▂▃▃▂▅▃▄▁▅▇▂▃
wandb:      val/t3/Poly_MMD █▅▄▄▄▃▃▄▂▃▂▃▂▃▂▂▂▂▂▁▂▃▂▂▂▃▂▂▁▂▃▁▂▂▂▁▃▃▁▂
wandb:       val/t3/RBF_MMD ▃▁▆▄▅▄▃▇▂▂▂▄▃▄▄▃▁▂▂▁▅▇▃▃▄▄▃▄▂▂▅▂▄▃▄▁▇█▃▃
wandb: val/t4/1-Wasserstein █▆▅▄▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▁▂▂▂▁▂▂▁▁
wandb: val/t4/2-Wasserstein █▅▃▂▃▂▂▄▂▂▁▂▄▂▂▂▃▂▂▁▂▃▁▃▂▂▂▂▂▃▂▂▅▂▂▁▂▄▂▂
wandb:    val/t4/Linear_MMD █▆▄▂▄▃▃▅▃▂▂▃▅▃▃▃▃▂▃▁▃▃▂▄▃▃▂▃▂▄▃▃▆▃▃▁▃▅▂▃
wandb:       val/t4/Mean_L2 █▅▅▃▄▄▃▄▃▃▂▄▄▂▃▃▃▁▂▁▃▃▁▄▂▃▂▂▂▃▂▃▅▁▂▁▂▄▁▂
wandb:      val/t4/Mean_MSE █▅▅▃▄▄▄▄▃▃▂▄▃▂▃▃▃▂▃▁▃▃▁▄▃▃▂▃▂▃▂▃▅▁▂▁▂▅▁▃
wandb:      val/t4/Poly_MMD █▆▄▂▄▃▃▅▃▂▂▃▅▃▃▄▄▂▃▁▄▃▂▄▃▃▂▃▂▄▃▃▆▂▃▁▃▅▂▃
wandb:       val/t4/RBF_MMD █▅▄▃▃▃▃▃▂▂▂▃▃▂▂▂▃▂▂▁▂▃▁▄▂▃▂▂▁▃▂▂▅▁▂▁▂▄▁▂
wandb: 
wandb: Run summary:
wandb:                epoch 1161
wandb:   leaveout_timepoint -1.0
wandb:           train/loss 0.07822
wandb:  trainer/global_step 15105
wandb:             val/loss 0.07901
wandb: val/t1/1-Wasserstein 0.82465
wandb: val/t1/2-Wasserstein 0.8502
wandb:    val/t1/Linear_MMD 0.00213
wandb:       val/t1/Mean_L1 0.04727
wandb:       val/t1/Mean_L2 0.06004
wandb:      val/t1/Mean_MSE 0.0036
wandb:     val/t1/Median_L1 0.00292
wandb:     val/t1/Median_L2 0.89027
wandb:    val/t1/Median_MSE 0.86582
wandb:      val/t1/Poly_MMD 0.04611
wandb:       val/t1/RBF_MMD 0.03827
wandb: val/t2/1-Wasserstein 0.054
wandb: val/t2/2-Wasserstein 0.04119
wandb:    val/t2/Linear_MMD 0.00726
wandb:       val/t2/Mean_L1 0.00106
wandb:       val/t2/Mean_L2 0.85007
wandb:      val/t2/Mean_MSE 0.81814
wandb:     val/t2/Median_L1 0.00125
wandb:     val/t2/Median_L2 0.02921
wandb:    val/t2/Median_MSE 0.03253
wandb:      val/t2/Poly_MMD 0.08523
wandb:       val/t2/RBF_MMD 0.06493
wandb: val/t3/1-Wasserstein 0.03531
wandb: val/t3/2-Wasserstein 0.0326
wandb:    val/t3/Linear_MMD 0.83116
wandb:       val/t3/Mean_L1 0.00294
wandb:       val/t3/Mean_L2 0.03775
wandb:      val/t3/Mean_MSE 0.04561
wandb:     val/t3/Median_L1 0.83494
wandb:     val/t3/Median_L2 0.04435
wandb:    val/t3/Median_MSE 0.05422
wandb:      val/t3/Poly_MMD 0.85101
wandb:       val/t3/RBF_MMD 0.00208
wandb: val/t4/1-Wasserstein 0.86039
wandb: val/t4/2-Wasserstein 0.00205
wandb:    val/t4/Linear_MMD 0.04456
wandb:       val/t4/Mean_L2 0.04729
wandb:      val/t4/Mean_MSE 0.0587
wandb:      val/t4/Poly_MMD 0.0366
wandb:       val/t4/RBF_MMD 0.00376
wandb: 
wandb: 🚀 View run atomic-forest-1 at: https://wandb.ai/allepalma/eb_geodesic/runs/rfd2soe8
wandb: ️⚡ View job at https://wandb.ai/allepalma/eb_geodesic/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1MTg0MzQwMA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: /ictstr01/home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/eb_geodesic/wandb/run-20240323_213250-rfd2soe8/logs
