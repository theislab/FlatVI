WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1695462353.848244  803743 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.
  self.seed = seed
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.
  self.dl_pin_memory_gpu_training = (
wandb: Currently logged in as: allepalma. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/3_OFFICIAL_cfm_pancreas_latent_vae_high_d/wandb/run-20230923_114613-2ac9osl4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-tree-1
wandb: ⭐️ View project at https://wandb.ai/allepalma/3_OFFICIAL_cfm_pancreas_latent_geodesic_high_d
wandb: 🚀 View run at https://wandb.ai/allepalma/3_OFFICIAL_cfm_pancreas_latent_geodesic_high_d/runs/2ac9osl4
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_cfm.py datamodule=CFM_pancreas_geodesic_high l ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_cfm.py datamodule=CFM_pancreas_geodesic_high l ...
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /lustre/groups/ml01/workspace/alessandro.palma/scCFM/experiments/3_OFFICIAL_cfm_pancreas_latent_vae_high_d/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type      | Params
----------------------------------------
0 | net       | MLP       | 11.2 K
1 | node      | NeuralODE | 11.2 K
2 | criterion | MSELoss   | 0     
----------------------------------------
11.2 K    Trainable params
0         Non-trainable params
11.2 K    Total params
0.045     Total estimated model params size (MB)
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/icb/alessandro.palma/miniconda3/envs/scCFM/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (33) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:          train/loss ██▇▃▆▅▅▄▇▄▆▄▅▄▄▃▂▄▂▄▃▂▃▃▂▃▂▂▄▁▄▂▂▁▂▂▃▃▃▃
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   val/1-Wasserstein ██▆▆▄▃▃▄▃▃▃▃▃▃▃▂▃▃▃▃▃▃▂▂▃▂▃▂▂▁▁▂▂▃▃▂▃▂▃▂
wandb:   val/2-Wasserstein ██▅▆▄▃▃▄▃▃▃▃▂▂▃▂▃▃▃▃▃▃▂▂▂▂▃▂▂▁▁▂▂▃▃▁▃▂▃▂
wandb:      val/Linear_MMD ▂▅▁▅▄▅▄▇▂▇▂▅▁▂▅▂▅▅▂▁▅▄▅▁▂▅▅▁▅▁▂▃▂█▅▄▇▄▄▄
wandb:         val/Mean_L1 ▂▇▂▅▃▅▄▆▂▇▃▇▂▂▄▁▇▄▁▂▄▃▄▂▂▄▂▃▅▂▁▃▃▇█▁▅▃▂▆
wandb:         val/Mean_L2 ▂█▁▄▂▆▃▆▃▇▃▆▁▂▃▁▇▃▁▂▄▄▃▂▁▅▃▁▅▂▁▂▂▆▇▂▆▄▂▅
wandb:        val/Mean_MSE ▂█▁▄▂▅▃▅▂▇▂▅▁▂▃▁▇▃▁▂▄▃▂▁▁▄▃▁▅▁▁▂▂▆▆▁▆▄▂▅
wandb:        val/Poly_MMD ▂▅▁▅▅▅▅▇▂▇▃▆▂▂▆▂▅▆▃▁▆▄▆▁▃▅▅▁▆▁▂▃▂█▆▄▇▅▅▅
wandb:         val/RBF_MMD ▃▄▃▆▆▅▅▆▁█▄▆▂▃▆▂▅▆▄▂▅▄▆▂▄▅▅▃▆▃▃▄▄█▆▄▇▄▅▅
wandb:            val/loss █▇▆▅▅▅▅▄▄▄▅▃▃▂▃▂▃▃▃▂▂▃▂▂▁▂▂▂▁▂▁▂▁▁▂▂▁▂▁▂
wandb: 
wandb: Run summary:
wandb:               epoch 582
wandb:          train/loss 0.07163
wandb: trainer/global_step 19238
wandb:   val/1-Wasserstein 1.11245
wandb:   val/2-Wasserstein 1.12626
wandb:      val/Linear_MMD 0.00056
wandb:         val/Mean_L1 0.01582
wandb:         val/Mean_L2 0.02277
wandb:        val/Mean_MSE 0.00052
wandb:        val/Poly_MMD 0.02359
wandb:         val/RBF_MMD 0.01912
wandb:            val/loss 0.07117
wandb: 
wandb: 🚀 View run bumbling-tree-1 at: https://wandb.ai/allepalma/3_OFFICIAL_cfm_pancreas_latent_geodesic_high_d/runs/2ac9osl4
wandb: ️⚡ View job at https://wandb.ai/allepalma/3_OFFICIAL_cfm_pancreas_latent_geodesic_high_d/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMDE2MDgxOQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/icb/alessandro.palma/environment/scCFM/project_dir/experiments/3_OFFICIAL_cfm_pancreas_latent_vae_high_d/wandb/run-20230923_114613-2ac9osl4/logs
